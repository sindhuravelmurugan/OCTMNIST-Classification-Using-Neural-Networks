# OCTMNIST-Classification-Using-Neural-Networks

## ğŸ“Œ Project Overview
This project focuses on **classifying OCTMNIST images** using a **Neural Network**. The model was optimized using various **training enhancement techniques** such as **Early Stopping, Learning Rate Scheduling (LRS), Gradient Accumulation, and Data Augmentation** to improve performance.

---

## ğŸ“‚ Dataset
The **OCTMNIST dataset** contains optical coherence tomography (OCT) images categorized into different classes. The objective is to build a **robust classification model** that can effectively distinguish between different eye conditions.

### **Preprocessing Steps:**
- Resized images for uniform input shape.
- Applied **normalization** and **data augmentation** where applicable.
- Converted labels into categorical format for multi-class classification.

---

## ğŸ› ï¸ Tech Stack
- **Programming Language:** Python  
- **Deep Learning Framework:** TensorFlow / PyTorch  
- **Libraries Used:**
  - `TensorFlow / PyTorch` â€“ Model implementation
  - `matplotlib & seaborn` â€“ Data visualization
  - `numpy & pandas` â€“ Data handling
  - `scikit-learn` â€“ Performance evaluation

---

## ğŸ” Model Training & Optimization Techniques

### **1ï¸âƒ£ Base Neural Network Model**
- Implemented a **basic CNN architecture** for image classification.
- Used **ReLU activation** and **Softmax for final classification**.

### **2ï¸âƒ£ Performance Enhancement Techniques**
âœ… **Early Stopping** â€“ Stopped training when validation loss stopped improving.  
âœ… **Learning Rate Scheduling (LRS)** â€“ Adjusted learning rates dynamically to enhance training.  
âœ… **Gradient Accumulation** â€“ Accumulated gradients over multiple batches to stabilize training.  
âœ… **Data Augmentation** â€“ Applied random transformations to increase dataset diversity.

---

## ğŸ“Š Results & Analysis

| Optimization Method       | Accuracy  |
|--------------------------|-----------|
| **Early Stopping**       | **0.81**  |
| **Learning Rate Scheduler (LRS)** | **0.79**  |
| **Gradient Accumulation** | **0.76**  |
| **Data Augmentation**     | **0.71**  |

ğŸ”¹ **Early Stopping & Learning Rate Scheduling** proved to be the most effective for this task.  
ğŸ”¹ **Data Augmentation reduced accuracy**, suggesting excessive noise or variation in this dataset.

---

ğŸš€ This is a showcase repository for my project **OCTMNIST Classification Using Neural Networks**.
The source code is currently private for personal reasons.

